<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: docs | aerenchyma]]></title>
  <link href="http://aerenchyma.github.io/blog/categories/docs/atom.xml" rel="self"/>
  <link href="http://aerenchyma.github.io/"/>
  <updated>2014-12-26T15:48:54-05:00</updated>
  <id>http://aerenchyma.github.io/</id>
  <author>
    <name><![CDATA[Jackie Cohen]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A useful Ruby script (part I)]]></title>
    <link href="http://aerenchyma.github.io/blog/2013/04/15/a-useful-ruby-script-for-mapping-projects/"/>
    <updated>2013-04-15T11:46:00-04:00</updated>
    <id>http://aerenchyma.github.io/blog/2013/04/15/a-useful-ruby-script-for-mapping-projects</id>
    <content type="html"><![CDATA[<h2>once upon a time</h2>

<p>in late 2012, I was at a bar with some friends, but the night ended with some excitement.</p>

<p>Soon after I left, I began writing a script in order to put together .csv files of locations, names, and phone numbers of various types of sites within a certain radius! The exclamation point there is a little facetious, but the excitement isn't really. I'm writing about this largely because I think its greatest value was in its original, unpolished, somewhat inefficient state: what's inefficient to a computer is not necessarily so to a human, and it's easy for a programmer to forget that.</p>

<p>This is a project that's taken place in a couple parts, and due to various pending Terms-of-Service (TOS) agreements and new <a href="http://en.wikipedia.org/wiki/Api">API</a> developments, I won't be publishing the final version of the code online <em>yet.</em></p>

<p><a href="http://aerenchyma.github.io/blog/2013/04/15/a-useful-ruby-script-for-mapping-projects/">Part I</a> (this post) will address the basic technical architecture of writing a Ruby script to speed up data-gathering for a non-profit organization and the initial questions that led to the script writing. <a href="">Part II</a> (to come) will address the whys -- specifically, why Ruby worked pretty well for this project and what I found to be useful takeaways from it. I am especially interested in takeaways with respect to teaching programming, ways of looking at projects as a developer, and application of technical skills as a service. <a href="">Part III</a> (to come) will address further technical possibilities -- what should work better, and what further questions this generated.</p>

<!-- more -->


<h2>initiation</h2>

<p>"Is there a faster way?" my friend asked, wondering if there were a way to automate some data gathering her organization was doing. <em>Is there a faster way?</em>  The answer is usually yes.</p>

<p>My friend and the organization she worked for wanted a faster way to gather data for a mapping project: for example, amidst this project, they might want to gather .csv files that held lists of places of worship (e.g. churches, mosques, temples...), within the tri-county area of Southeastern Michigan. They would analyze and separate these data in various ways and upload the final files for a given search to <a href="http://www.arcgis.com/about/">ArcGIS</a>, in order to create maps.</p>

<p>The problems?</p>

<ol>
<li><p><strong>Getting the data was hard:</strong>
In some cases for mapping sites of given types or categories, the data was readily available. The governent provides data files, including GIS shape files, with a lot of publicly available information; beyond things like that, there are available lists from many public sources; lists of public schools and locations, for example. But it's hard to find a list of all <em>places of worship</em>, say, because though this may be one category at a given level, on an organizational level these belong to many different categories: churches of a given denomination and Sikh Temples are usually not listed in the same public dataset, especially not throughout the tri-county area.</p></li>
<li><p><strong>Getting the data was slow:</strong>
The way they were currently approaching these problems, she explained, was making educated searches for terms (e.g. places of worship) that weren't easily correlated in your average public database and which didn't have an overarching organization. When the resulting data wasn't available in a database or google spreadsheet form, they copied them into Excel and spot-checked them for accuracy, going back and searching. They were working on a lot of projects simultaneously and this process was turning into quite a timesink for some.</p></li>
</ol>


<p>Was there a faster way? <em>Yes</em>, I said.</p>

<p>There are many services, both specific to the state of Michigan or metro Detroit and nationally or internationally that provide open source mapping and data services: <a href="http://datadrivendetroit.org/">Data Driven Detroit</a>, <a href="http://datakind.org/">DataKind</a>, <a href="http://www.openstreetmap.org/">Open Street Map</a>, among others. There are also many technical solutions to any problem of creating maps, but for those (like me) immersed in technical solutions, it is easy to forget the time and energy it takes to reach the baseline for these tools (for example, <a href="http://postgis.net/">PostGIS</a>).<sup id='fnref:1'><a href='#fn:1' rel='footnote'>1</a></sup></p>

<p>However, some (not all) of these services are prohibitively expensive, some require a more intermediate level of technical skill for productive use than her organization could spare or hire, and most are dependent upon data collections already existing. Thus, her question. She and others were spending a lot of time copying and pasting that didn't need to be spent. The missing piece for a quicker finish for their project was a short -- but admittedly sharp -- learning curve. <em>You can write a script to do that</em>, I said, or at least thought. <em>I can write this -- it'll be fun.</em></p>

<p>Okay.</p>

<h2>first steps &amp; ethical questions</h2>

<p>The next question for me, the script-writer, then, was: is there a service that provides this information in a useful way, and do they have an API? The initial answers to this ("sort of") are what keep me from posting code in its entirety (and leave me hoping that things will soon improve), but common sense took me a step further. Neither the Google Places API nor the Maps API on their own provided results fine-grained enough for the numbers we were looking for, knowing that, for example, there are somewhere (give or take large numbers) around 4500 places of worship in the <a href="http://en.wikipedia.org/wiki/Metro_Detroit">tri-county area</a> of Southeastern Michigan. Okay -- what next?</p>

<p>Well, scraping, while useful, has the potential of stepping over TOS boundaries, not something we want -- on the "client" end as an organization, nor as a developer. Nor indeed as a decent person, I guess, though I admit this judgment could depend somewhat on the TOS. (Heh.) But spending potentially-hundreds of hours copying and pasting leads to <em>some</em> (not all) of the same ends as scraping does.</p>

<p>If public data is being shared, and it's not being used commercially<sup id='fnref:2'><a href='#fn:2' rel='footnote'>2</a></sup> , it is probably being gotten and used all the time. That's what we use a phone book for, the ones that the city leaves on our doorsteps in Ann Arbor: say, where are the nearest couple churches? Well...</p>

<p>Whatif we access a big service provider, I thought -- one that wouldn't have to <em>worry</em> about funding bandwidth for a hit of a few seconds, with engineered breaks -- what if we only hit the server once, really? Instead of leaving people to copy and paste for weeks and weeks? Say I save a sample couple html pages for testing. They just need the data faster -- they need to get their project done, and they would like to make it efficient. I can write a script that automates that human effort. Maybe they'll hit it once, or twice, thrice, not too quickly, not even within the same day, and do the rest of the analysis offline.<sup id='fnref:3'><a href='#fn:3' rel='footnote'>3</a></sup></p>

<h2>architecture basics</h2>

<p>I used the <a href="http://mechanize.rubyforge.org/">Mechanize</a> and <a href="http://nokogiri.org/">Nokogiri</a> gems for the original script: Mechanize to submit appropriate query/ies to forms, Nokogiri to parse the results displayed on webpages, and the rest is all scripting in Ruby 1.9.3. (1.9.x Ruby was important here, because of its <a href="http://www.igvita.com/2009/02/04/ruby-19-internals-ordered-hash/">ordered hashes</a>.)</p>

<p>First, initialization of a Mechanize agent:</p>

<p><code>ruby
$agent = Mechanize.new
</code></p>

<p>I used this agent in several smaller scopes and thus (yes, I can hear the scolding; I accept it) made it global for the moment.</p>

<p>Then, saving a query in a variable like so:</p>

<p><code>ruby
query = "community center" # example
</code></p>

<p>Then, I access the page's forms to allow me to search for a query (in this case, the first form on the page was the relevant one for information we were interested in). For example,</p>

<p><code>ruby
page = $agent.get('http://YOURURL.com')
forms = page.forms
searchform = forms.first
searchform.search_terms = query
results = $agent.submit(searchform)
start_pg = results.uri # this is the page to start scraping from
</code></p>

<p>(Note that <strong>search_terms</strong> here is specific to the form. You'll want to take a look at the form(s), perhaps by <code>forms.each do {|f| p f}</code> in the console, to see what the appropriate entry box is <em>named</em> on the web form you're trying to enter a query in.</p>

<p>Also note that a form may have several fields you need to enter -- say, a search term and location -- and you'll need to look for all of those fields and write a line similar to that in line 4 in the snippet above for each.)</p>

<p>Immediately after this, I throw the results into a Nokogiri document for ease of parsing HTML to pull out relevant information, since the result web pages are intended for end-user viewers and not in easy-to-parse XML or JSON like you would get from the result of an API.</p>

<p><code>
pg = Nokogiri::HTML(open(start_pg)) # Nokogiri document of the start page
</code></p>

<p>Next, I have a <code>create_hashes(page)</code> function which expects a Nokogiri document (e.g. <code>pg</code> ) as a parameter. This function uses Nokogiri functionality to grab different pieces of information that we cared about -- e.g. the name of a given place, its address, its phone number. The function handles exceptions and keeps count of the individual results gathered.<sup id='fnref:4'><a href='#fn:4' rel='footnote'>4</a></sup></p>

<p>So <code>create_hashes</code> calls another function to make sure no duplicate entries are included in any set of .csv files corresponding to the the same search. It then pulls all associated pieces of information into a Ruby hash data structure and adds that to a Ruby array of hashes, each of which corresponds to one result (one church, for example).</p>

<p>For instance, one hash might look like:</p>

<p><code>ruby
{ "name" =&gt; "Generic Church Name", "addr" =&gt; "1000 Streetname Dr", "city" =&gt; "Detroit", "state" =&gt; "MI", "zip" =&gt; "48126", "phone" =&gt; "555-555-5555" }
</code></p>

<p><font size="-3">n.b. that is NOT a real address or phone number</font></p>

<p>I have a function <code>transform_hash(hn)</code>, where <code>hn</code> is a hash corresponding to a single result, which transforms one of the aggregated hashes into a line in a .csv file. In the case of the information I was gathering, that looked like:</p>

<p><code>ruby
def transform_hash(hn)
  s = "#{hn['name']}, #{hn['addr']}, #{hn['city']}, #{hn['state']}, #{hn['zip']}, #{hn['phone']}\n"
  s
end
</code></p>

<p>And I make sure that every .csv file created has the form of {date search occurred}-{location for search}-{search query}{#, if there are > 1000 unique results}, so I create a filename and open a new file with that filename for writing.</p>

<p><font size="-3">( <code>today_string()</code> is a function which creates a string of the current date in the string format I wanted: YYYY-MM-DD)</font></p>

<p><code>ruby
fname = today_string() + "-" + location.gsub!(",","").split(' ').map {|w| CGI.escape(w)}.join("") + query.split(' ').map {|w| CGI.escape(w)}.join("")
</code></p>

<p>And finally, the script writes .csv files as appropriate, starting a new file with the header ( <code>Name, Address, City, State, Phone</code> ) each time one file reaches 990 entries (AKA 991 lines).</p>

<h2>discussion</h2>

<p>This script I typed out on first go, post the original discussion with my friend, is not exactly the code I'm most proud of from a technical perspective: programmatically, I don't think there's a great deal to learn for someone who is already confident with web programming in Ruby.</p>

<p>So while I’m always pleased to write good-smelling code<sup id='fnref:5'><a href='#fn:5' rel='footnote'>5</a></sup>, I honestly saw this whole scenario through rose-colored glasses because it <em>worked</em>, even though I didn’t spend a ton of hours honing it; I knew there was a possibility here so I sat down and wrote it and ran it and it helped, and that was what mattered.</p>

<p>My friend and her team needed something that worked better (in this case, “better” meant faster – and potentially more reliable) than their previous method(s) of data-gathering. I believe this solution satisfied that without crawling rudely or putting undue weight anywhere except an individual computer for a minute or so. (That is to say, it wasn't as efficient as it really ought to be, and while it is a sustainable solution in some senses, it isn't in others. More on this to follow.)</p>

<p>And thus I like this as an example of a good <em>use</em> of programming skill. A web app for this is in progress, but the process of hacking together a solution to this problem was valuable to me as a programmer.</p>

<p>Is there a faster way? Yes. This script brought down the team's time expectation for the mapping project from months to days.</p>

<p>And there are lessons to be gained for your average programmer from this small project. Maybe I'd wince to ship this and maybe I wouldn't, but in this particular case, it <em>should</em> have been shipped: it did all it needed to do for the small scope we assigned it.<sup id='fnref:6'><a href='#fn:6' rel='footnote'>6</a></sup> It also made me think seriously about what important questions are for a developer when the end-user is someone running a (simple? perhaps yes!) script on the command line, especially if that person is not necessarily <em>comfortable</em> with its use.</p>

<p>What's the input? What's the output? How does the API work? What's the easiest solution, the fastest solution, the most elegant?</p>

<p>It's easy to get caught up in jargon just answering these questions. While there is <em>usually</em> reason to have these in-depth discussions, there do exist occurrences when they are a hinderance and not a help, at least not right away. Not a lot of programmers or computer scientists work in the non-profit industry overall, especially not in jobs where technology isn't the end goal of their particular projects. But programming skill can still be valuable there, and these kind of caveats (what questions not to ask, which performance issues (not) to worry about first...) are useful to keep in mind.</p>

<hr />

<p>Find <a href="">Part II</a> and <a href="">Part III</a></p>

<hr />

<p><div class="footnotes">
	<ol>
		<li id='fn:1'><p> I'll come back to this issue in Part II
<a href='#fnref:1' rev='footnote'>↩</a></p>
</li><li id='fn:2'><p> Arguably, not even in other creative works -- but this is a whole 'nother issue so we'll leave that aside for the moment
<a href='#fnref:2' rev='footnote'>↩</a></p>
</li><li id='fn:3'><p> Note: no TOS was violated in the making of this script, but it is somewhat unclear for the current incarnation, thus the pause and non-public full code.
<a href='#fnref:3' rev='footnote'>↩</a></p>
</li><li id='fn:4'><p> The count is kept specifically because the results were intended for use with ArcGIS, which, for the organization's current tools, worked best with .CSV files of 1000 lines or fewer.
<a href='#fnref:4' rev='footnote'>↩</a></p>
</li><li id='fn:5'><p> <a href="http://en.wikipedia.org/wiki/Code_smell">On code smell</a> -- thanks to David Albert for my introduction to the term
<a href='#fnref:5' rev='footnote'>↩</a></p>
</li><li id='fn:6'><p> I still welcome what code reviews are possible without posting most of the code, and/or comments or suggestions. It <em>will</em> all be open-source under the MIT license.
<a href='#fnref:6' rev='footnote'>↩</a></p>
</li>
	</ol>
</div>
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Open.Michigan Analytics, the what]]></title>
    <link href="http://aerenchyma.github.io/blog/2013/02/18/open-dot-michigan-analytics/"/>
    <updated>2013-02-18T17:05:00-05:00</updated>
    <id>http://aerenchyma.github.io/blog/2013/02/18/open-dot-michigan-analytics</id>
    <content type="html"><![CDATA[<p>Starting in January 2013, I have been a software developer at the <a href="http://open.umich.edu">Open.Michigan Initiative</a>, which is housed within the University of Michigan Medical School. Open.Michigan (OM) promotes open licensing and open educational resources (OER), and we publish OER on our website, which is run on the <a href="https://github.com/openmichigan/OERbit">OERbit</a> framework, which in turn is based on <a href="http://drupal.org/">Drupal</a>.</p>

<p><small>(True to our goals, all the code we produce is open source, free for use and adaptation with attribution. See licenses.)</small></p>

<p>My first project is Open.Michigan Analytics -- accessing, displaying, and allowing access to analytics for the resources we publish. In twelve words or fewer. Hah. I am documenting the process, including decisions and missteps as well as step-by-steps of enacted processes, especially because this includes my progress learning Drupal (I am flexible with languages, but I have most experience developing in Python, and I had never worked even semi-professionally in PHP before this project), and adaptations and contributions to a fairly new open source project, examples of which are good to have. Also, if anyone picks this up inside or outside our office, I'd like to have documentation of the development (and not just its use) around.</p>

<p>Below, I set out the original planning stages and goals for the project. Soon: the first decisions made, setup issues, and options I set out for proceeding.</p>

<!-- more -->


<hr />

<h4>01/22/2013:</h4>

<h3>Original plan, analytics project</h3>

<p><em>Our original outline of the project plan, in phases</em></p>

<h3><em>Phase I</em></h3>

<ul>
<li><strong>Goal:</strong> Display (publically) specific analytics on a given course/resource page.</li>
<li><strong>Primary audience:</strong> authors of content, and U/M leadership.</li>
<li><strong>Motivation:</strong> use analytics display as way of promoting creation of open content?

<ul>
<li>Being able to show results of open content creation and publishing to content-creators.</li>
<li>"This has been viewed this many times..."</li>
</ul>
</li>
<li><strong>Substance:</strong>

<ul>
<li>Google Analytics:

<ul>
<li>total zip downloads #</li>
<li>most downloaded material (of materials included in a course/resource on OERbit)</li>
<li>total views of course/resource</li>
<li>date last material added (?) n.b. this info included in "date last modified" field on page</li>
</ul>
</li>
<li>YouTube Analytics:

<ul>
<li>total youtube views (of videos for a given course/resource)</li>
<li>total youtube comments (#, potentially store data, depending upon TOS)</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3><em>Phase II</em></h3>

<ul>
<li><strong>Goal:</strong> flesh out display and available downloads</li>
<li><strong>Primary audience:</strong> same as <em>Phase I</em></li>
<li><strong>Motivation:</strong> same as above, with additonal possibility of content creators drilling deeper into analytics, personal results of content-creation, a better understanding for OM wrt which access points are used in what ways, which forms of material seem to be used most often, etc.</li>
</ul>


<p>[Below is significantly paraphrased from our original internal outline.]</p>

<ul>
<li><strong>Substance:</strong>

<ul>
<li><em>Display, for a given course/resource:</em>

<ul>
<li>Social media stuff</li>
<li><a href="http://deepblue.lib.umich.edu/">Deep Blue</a> downloads/views</li>
<li>Wikipedia stuff</li>
<li>number of countries accessed</li>
<li>geographic views</li>
<li>Downloads -- YouTube comments (pending TOS), demographics, site demographics</li>
</ul>
</li>
</ul>
</li>
</ul>


<h3><em>Phase III</em></h3>

<ul>
<li><strong>Goal:</strong> increased availability of and access to analytics information</li>
<li><strong>Potentially:</strong>

<ul>
<li>integrate iTunes U</li>
<li><a href="http://www.carma.umich.edu/">CARMA</a> views</li>
<li>more download options</li>
<li>further API options</li>
</ul>
</li>
<li><strong>Considerations:</strong> API updates, integration of APIs, structure of personal metadata and paradata, relation to other projects, modularity, documentation forms, UI design</li>
</ul>


<p>This plan was soon substantially revised -- so exciting, right? More to come; drafts in progress. (Internal documentation + development does come first.)</p>
]]></content>
  </entry>
  
</feed>
