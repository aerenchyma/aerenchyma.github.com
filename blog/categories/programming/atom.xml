<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: programming | everything's interesting]]></title>
  <link href="http://aerenchyma.github.com/blog/categories/programming/atom.xml" rel="self"/>
  <link href="http://aerenchyma.github.com/"/>
  <updated>2013-04-17T22:54:22-04:00</updated>
  <id>http://aerenchyma.github.com/</id>
  <author>
    <name><![CDATA[Jackie Cohen]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[A useful Ruby script (part I)]]></title>
    <link href="http://aerenchyma.github.com/blog/2013/04/15/a-useful-ruby-script-for-mapping-projects/"/>
    <updated>2013-04-15T11:46:00-04:00</updated>
    <id>http://aerenchyma.github.com/blog/2013/04/15/a-useful-ruby-script-for-mapping-projects</id>
    <content type="html"><![CDATA[<h2>once upon a time</h2>

<p>in late 2012, I was at a bar with some friends, but the night ended with some excitement.</p>

<p>Soon after I left, I began writing a script in order to put together .csv files of locations, names, and phone numbers of various types of sites within a certain radius! The exclamation point there is a little facetious, but I'm not kidding about the excitement. I love information retrieval and teaching programming for use to improve efficiency. I like scripting and any chance to share how cool it is; this was all of the above.</p>

<p>This is a project that's taken place in several parts, and due to various pending Terms-of-Service (TOS) agreements and new <a href="http://en.wikipedia.org/wiki/Api">API</a> developments, I won't be publishing the final version of the code online <em>yet.</em> But I'm outlining this process here anyhow because I think writing about code is good.</p>

<p><a href="http://aerenchyma.github.io/blog/2013/04/15/a-useful-ruby-script-for-mapping-projects/">Part I</a> (this post) will address the basic technical architecture of writing a Ruby script to speed up data-gathering for a local non-profit organization, use of particular Ruby gems, etc (and the initial questions that led to the script writing).</p>

<p><a href="">Part II</a> (to come) will address the whys -- specifically, why Ruby worked well for this project and what I found to be useful takeaways from it. I am especially interested in takeaways with respect to teaching programming, ways of looking at projects as a developer, and application of technical skills as a service.</p>

<p><a href="">Part III</a> (to come) will address further technical possibilities -- what should work better, what tools are future possibilities, what questions this generated, and a clear outline of what I've learnt (from a programming perspective) by working on this.</p>

<!-- more -->


<h2>initiation</h2>

<p>"Is there a faster way?" my friend asked, wondering if there were a way to automate some data gathering her organization was doing. <em>Is there a faster way?</em>  The answer is usually yes.</p>

<p>My friend and the organization she worked for wanted a faster way to gather data for a mapping project: for example, amidst this project, they might want to gather .csv files that held lists of places of worship (e.g. churches, mosques, temples...), within the tri-county area of Southeastern Michigan. They would analyze and separate these data in various ways and upload the final files for a given search to <a href="http://www.arcgis.com/about/">ArcGIS</a>, in order to create maps.</p>

<p>The problems?</p>

<ol>
<li><p><strong>Getting the data was hard:</strong>
In some cases for mapping sites of given types or categories, the data was readily available. The governent provides data files, including GIS shape files, with a lot of publicly available information; beyond things like that, there are available lists from many public sources; lists of public schools and locations, for example. But it's hard to find a list of all <em>places of worship</em>, say, because though this may be one category at a given level, on an organizational level these belong to many different categories: churches of a given denomination and Sikh Temples are usually not listed in the same public dataset, especially not throughout the tri-county area.</p></li>
<li><p><strong>Getting the data was slow:</strong>
The way they were currently approaching these problems, she explained, was making educated searches for terms (e.g. places of worship) that weren't easily correlated in your average public database and which didn't have an overarching organization. When the resulting data wasn't available in a database or google spreadsheet form, they copied them into Excel and spot-checked them for accuracy, going back and searching. They were working on a lot of projects simultaneously and this process was turning into quite a timesink for some.</p></li>
</ol>


<p>Was there a faster way? <em>Yes</em>, I said.</p>

<p>There are many services, both specific to the state of Michigan or metro Detroit and nationally or internationally that provide open source mapping and data services: <a href="http://datadrivendetroit.org/">Data Driven Detroit</a>, <a href="http://datakind.org/">DataKind</a>, <a href="http://www.openstreetmap.org/">Open Street Map</a>, among others. There are also many technical solutions to any problem of creating maps, but for those (like me) immersed in technical solutions, it is easy to forget the time and energy it takes to reach the baseline for these tools (for example, <a href="http://postgis.net/">PostGIS</a>).<sup id='fnref:1'><a href='#fn:1' rel='footnote'>1</a></sup></p>

<p>However, some (not all) of these services are prohibitively expensive, some require a more intermediate level of technical skill for productive use than her organization could spare or hire, and most are dependent upon data collections already existing. Thus, her question. She and others were spending a lot of time copying and pasting that didn't need to be spent. The missing piece for a quicker finish for their project was a short -- but admittedly sharp -- learning curve. <em>You can write a script to do that</em>, I said, or at least thought. <em>I can write this -- it'll be fun.</em></p>

<p>Okay.</p>

<h2>first steps &amp; ethical questions</h2>

<p>The next question for me, the script-writer, then, was: is there a service that provides this information in a useful way, and do they have an API? The initial answers to this ("sort of") are what keep me from posting code in its entirety (and leave me hoping that things will soon improve), but common sense took me a step further. Neither the Google Places API nor the Maps API on their own provided results fine-grained enough for the numbers we were looking for, knowing that, for example, there are somewhere (give or take large numbers) around 4500 places of worship in the <a href="http://en.wikipedia.org/wiki/Metro_Detroit">tri-county area</a> of Southeastern Michigan. Okay -- what next?</p>

<p>Well, scraping, while useful, has the potential of stepping over TOS boundaries, not something we want -- on the "client" end as an organization, nor as a developer. Nor indeed as a decent person, I guess, though I admit this judgment could depend somewhat on the TOS. (Heh.) But spending potentially-hundreds of hours copying and pasting leads to <em>some</em> (not all) of the same ends as scraping does.</p>

<p>If public data is being shared, and it's not being used commercially<sup id='fnref:2'><a href='#fn:2' rel='footnote'>2</a></sup> , it is probably being gotten and used all the time. That's what we use a phone book for, the ones that the city leaves on our doorsteps in Ann Arbor: say, where are the nearest couple churches? Well...</p>

<p>Whatif we access a big service provider, I thought -- one that wouldn't have to <em>worry</em> about funding bandwidth for a hit of a few seconds, with engineered breaks -- what if we only hit the server once, really? Instead of leaving people to copy and paste for weeks and weeks? Say I save a sample couple html pages for testing. They just need the data faster -- they need to get their project done, and they would like to make it efficient. I can write a script that automates that human effort. Maybe they'll hit it once, or twice, thrice, not too quickly, not even within the same day, and do the rest of the analysis offline.<sup id='fnref:3'><a href='#fn:3' rel='footnote'>3</a></sup></p>

<h2>architecture basics</h2>

<p>I used the <a href="http://mechanize.rubyforge.org/">Mechanize</a> and <a href="http://nokogiri.org/">Nokogiri</a> gems for the original script: Mechanize to submit appropriate query/ies to forms, Nokogiri to parse the results displayed on webpages, and the rest is all scripting in Ruby 1.9.3. (1.9.x Ruby was important here, because of its <a href="http://www.igvita.com/2009/02/04/ruby-19-internals-ordered-hash/">ordered hashes</a>.)</p>

<p>First, initialization of a Mechanize agent:</p>

<p><code>ruby
$agent = Mechanize.new
</code></p>

<p>I used this agent in several smaller scopes and thus (yes, I can hear the scolding; I accept it) made it global for the moment.</p>

<p>Then, saving a query in a variable like so:</p>

<p><code>ruby
query = "community center" # example
</code></p>

<p>Then, I access the page's forms to allow me to search for a query (in this case, the first form on the page was the relevant one for information we were interested in). For example,</p>

<p><code>ruby
page = $agent.get('http://YOURURL.com')
forms = page.forms
searchform = forms.first
searchform.search_terms = query
results = $agent.submit(searchform)
start_pg = results.uri # this is the page to start scraping from
</code></p>

<p>(Note that <strong>search_terms</strong> here is specific to the form. You'll want to take a look at the form(s), perhaps by <code>forms.each do {|f| p f}</code> in the console, to see what the appropriate entry box is <em>named</em> on the web form you're trying to enter a query in.</p>

<p>Also note that a form may have several fields you need to enter -- say, a search term and location -- and you'll need to look for all of those fields and write a line similar to that in line 4 in the snippet above for each.)</p>

<p>Immediately after this, I throw the results into a Nokogiri document for ease of parsing HTML to pull out relevant information, since the result web pages are intended for end-user viewers and not in easy-to-parse XML or JSON like you would get from the result of an API.</p>

<p><code>
pg = Nokogiri::HTML(open(start_pg)) # Nokogiri document of the start page
</code></p>

<p>Next, I have a <code>create_hashes(page)</code> function which expects a Nokogiri document (e.g. <code>pg</code> ) as a parameter. This function uses Nokogiri functionality to grab different pieces of information that we cared about -- e.g. the name of a given place, its address, its phone number. The function handles exceptions and keeps count of the individual results gathered.<sup id='fnref:4'><a href='#fn:4' rel='footnote'>4</a></sup></p>

<p>So <code>create_hashes</code> calls another function to make sure no duplicate entries are included in any set of .csv files corresponding to the the same search. It then pulls all associated pieces of information into a Ruby hash data structure and adds that to a Ruby array of hashes, each of which corresponds to one result (one church, for example).</p>

<p>For instance, one hash might look like:</p>

<p><code>ruby
{ "name" =&gt; "Generic Church Name", "addr" =&gt; "1000 Streetname Dr", "city" =&gt; "Detroit", "state" =&gt; "MI", "zip" =&gt; "48126", "phone" =&gt; "555-555-5555" }
</code></p>

<p><font size="-3">n.b. that is NOT a real address or phone number</font></p>

<p>I have a function <code>transform_hash(hn)</code>, where <code>hn</code> is a hash corresponding to a single result, which transforms one of the aggregated hashes into a line in a .csv file. In the case of the information I was gathering, that looked like:</p>

<p><code>ruby
def transform_hash(hn)
  s = "#{hn['name']}, #{hn['addr']}, #{hn['city']}, #{hn['state']}, #{hn['zip']}, #{hn['phone']}\n"
  s
end
</code></p>

<p>And I make sure that every .csv file created has the form of {date search occurred}-{location for search}-{search query}{#, if there are > 1000 unique results}, so I create a filename and open a new file with that filename for writing.</p>

<p><font size="-3">( <code>today_string()</code> is a function which creates a string of the current date in the string format I wanted: YYYY-MM-DD)</font></p>

<p><code>ruby
fname = today_string() + "-" + location.gsub!(",","").split(' ').map {|w| CGI.escape(w)}.join("") + query.split(' ').map {|w| CGI.escape(w)}.join("")
</code></p>

<p>And finally, the script writes .csv files as appropriate, starting a new file with the header ( <code>Name, Address, City, State, Phone</code> ) each time one file reaches 990 entries (AKA 991 lines).</p>

<h2>discussion</h2>

<p>This script I typed out on first go, post the original discussion with my friend, is not exactly the code I'm most proud of from a technical perspective: programmatically, I don't think there's a great deal to learn for someone who is already confident with web programming in Ruby.</p>

<p>So while I’m always pleased to write good-smelling code<sup id='fnref:5'><a href='#fn:5' rel='footnote'>5</a></sup>, I honestly saw this whole scenario through rose-colored glasses because it <em>worked</em>, even though I didn’t spend a ton of hours honing it; I knew there was a possibility here so I sat down and wrote it and ran it and it helped, and that was what mattered.</p>

<p>My friend and her team needed something that worked better (in this case, “better” meant faster – and potentially more reliable) than their previous method(s) of data-gathering. I believe this solution satisfied that without crawling rudely or putting undue weight anywhere except an individual computer for a minute or so. (That is to say, it wasn't as efficient as it really ought to be, and while it is a sustainable solution in some senses, it isn't in others. More on this to follow.)</p>

<p>And thus I like this as an example of a good <em>use</em> of programming skill. A web app for this is in progress, but the process of hacking together a solution to this problem was valuable to me as a programmer.</p>

<p>Is there a faster way? Yes. This script brought down the team's time expectation for the mapping project from months to days.</p>

<p>And there are lessons to be gained for your average programmer from this small project. Maybe I'd wince to ship this and maybe I wouldn't, but in this particular case, it <em>should</em> have been shipped: it did all it needed to do for the small scope we assigned it.<sup id='fnref:6'><a href='#fn:6' rel='footnote'>6</a></sup> It also made me think seriously about what important questions are for a developer when the end-user is someone running a (simple? perhaps yes!) script on the command line, especially if that person is not necessarily <em>comfortable</em> with its use.</p>

<p>What's the input? What's the output? How does the API work? What's the easiest solution, the fastest solution, the most elegant?</p>

<p>It's easy to get caught up in jargon just answering these questions. While there is <em>usually</em> reason to have these in-depth discussions, there do exist occurrences when they are a hinderance and not a help, at least not right away. Not a lot of programmers or computer scientists work in the non-profit industry overall, especially not in jobs where technology isn't the end goal of their particular projects. But programming skill can still be valuable there, and these kind of caveats (what questions not to ask, which performance issues (not) to worry about first...) are useful to keep in mind.</p>

<hr />

<p>Find <a href="">Part II</a> and <a href="">Part III</a></p>

<hr />

<p><div class="footnotes">
	<ol>
		<li id='fn:1'><p> I'll come back to this issue in Part II
<a href='#fnref:1' rev='footnote'>↩</a></p>
</li><li id='fn:2'><p> Arguably, not even in other creative works -- but this is a whole 'nother issue so we'll leave that aside for the moment
<a href='#fnref:2' rev='footnote'>↩</a></p>
</li><li id='fn:3'><p> Note: no TOS was violated in the making of this script, but it is somewhat unclear for the current incarnation, thus the pause and non-public full code.
<a href='#fnref:3' rev='footnote'>↩</a></p>
</li><li id='fn:4'><p> The count is kept specifically because the results were intended for use with ArcGIS, which, for the organization's current tools, worked best with .CSV files of 1000 lines or fewer.
<a href='#fnref:4' rev='footnote'>↩</a></p>
</li><li id='fn:5'><p> <a href="http://en.wikipedia.org/wiki/Code_smell">On code smell</a> -- thanks to David Albert for my introduction to the term
<a href='#fnref:5' rev='footnote'>↩</a></p>
</li><li id='fn:6'><p> I still welcome what code reviews are possible without posting most of the code, and/or comments or suggestions. It <em>will</em> all be open-source under the MIT license.
<a href='#fnref:6' rev='footnote'>↩</a></p>
</li>
	</ol>
</div>
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[instabitly]]></title>
    <link href="http://aerenchyma.github.com/blog/2013/03/25/instabitly/"/>
    <updated>2013-03-25T00:29:00-04:00</updated>
    <id>http://aerenchyma.github.com/blog/2013/03/25/instabitly</id>
    <content type="html"><![CDATA[<p>Quick drive-by: <a href="https://github.com/aerenchyma/instabitly">instabitly</a>, a script I wrote recently to import saved links from <a href="http://instapaper.com">Instapaper</a> to <a href="https://bitly.com/">bit.ly</a>.</p>

<p>I did it for personal reasons -- primarily because I want all this stuff stored redundantly. Obviously I haven't solved that problem in its entirety, but there were also other reasons. Because I care about things like this. There's a README. More, like more in the other documentation series I've discussed so far, will have to appear after recent projects calm down some.</p>

<p>Let me also take this opportunity to encourage you to <a href="http://www.instapaper.com/subscription">support Instapaper</a> if you can and it's a service you like/use.</p>

<p>In my imaginary free time, I have been trying to absorb all the <a href="http://pyvideo.org/category/33/pycon-us-2013">fantastic PyCon</a> <a href="https://speakerdeck.com/pyconslides">talks</a> (I have an active imagination).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[highlight of the job]]></title>
    <link href="http://aerenchyma.github.com/blog/2012/05/02/highlight-of-the-job/"/>
    <updated>2012-05-02T14:15:00-04:00</updated>
    <id>http://aerenchyma.github.com/blog/2012/05/02/highlight-of-the-job</id>
    <content type="html"><![CDATA[<p>On May first, I reached the end of Michigan's 2012 Winter semester, and with it the end of one of the best jobs. A student called the deadline for the final projects the "end of an era" -- that's what it felt like to us (the staff), too. ("It feels like we're doing twenty projects!")</p>

<p>Except that it's more thrilling to watch people work together and seriously, rapidly and usefully learn stuff, and help each other -- better to watch forty-some eureka moments than only have a few yourself.</p>

<!-- more -->


<p>I've been very lucky jobwise, this school year, overall. But the off-chance I got to IA for an intro programming class has lit up my life, in the worst, best, and cheesiest sense of the words. (I know, I make me feel ill, too.) Nothing, except living in a co-op, has taught me more about <i>people</i> and dealing with and reacting to others responsibly; in no job I've ever had, including 'being a full-time student', have I ever come home feeling like I did as many worthwhile things with my day (though certainly there are others where finishing weeks, or projects, has left me feeling as accomplished).</p>

<p>But I've rarely been as amazed as I often was in that lecture hall. I feel like I got to know more than fifty people this past semester, and when they left the exam, I was glad for them all, and my co-IA (who was brilliant) and I were, if anything, underestimating when we told them how proud we were and how much luck we wished for them in the future. I've never seen students leave an exam with so much glee and pride, and I've never been so sorry to see quite that many people go. (I didn't get along with <i>that</i> many people in my high school class... that's been a while.)</p>

<p>I've known for years that teaching someone else is one of the best ways to learn something thoroughly, but I didn't know the extent to which it would change how I thought about learning, and how I thought about myself. My students have taught me good lessons over and over again. Those who struggled with concepts and kept coming back, those who leaned over to help others without giving them the answers. "Let's break it down." Those who played music as office hours before the final crept later and later, and laughed when I danced (just a little). "Even the GSIs are getting into it!" Well, yeah -- we want to have fun too. (GSIs we weren't, but for all intents and purposes. It was a position I'm not used to being in, but I enjoyed it.)</p>

<p>I felt heading into the job that in terms of programming, a strength of mine was explaining programming concepts, because I struggled at first and then crested the -- what? the hill of difficulty? and have loved programming and building things with code ever since, but in my upper-level classes, I recognized my students' struggle on a new level and I sympathized, and I think that helped. Time-depth-wise, I was closer to their level than the rest of the staff team, by quite a lot, and though that made me nervous at the get-go, it turned out to be really valuable. I'm endlessly glad I was not the only staff member, but I work well in a team anyway (and I had an <i>awesome</i> co-IA). It is difficult to remember what it's like <i>not</i> to know stuff (e.g. what it means to store a Python dictionary in a variable, what a for-loop does), but I found the reach backwards easier and easier as I went on. As I told the professor for whom I worked, I found myself using more than anything else things I'd learnt while playing around on my own, and things I'd learnt in the class itself, more than I used knowledge from subsequent classes.</p>

<p>Every meaningful "thank you" was ridiculously valuable to me. Every moment of sudden understanding. The look on one girl's face as she got a web app working: "This is the best thing I've ever seen in my whole life; I'm so happy!" A guy's grin as he said "Oh! We're extracting information from the internet!" The genial frustration ("I'm back to make your life miserable, Jackie!" / "Never. What's up?"). The glee I came to expect as I whirled on a heel to say "YES! Exactly! So what type will this be?"</p>

<p>I wanted to make a pun about dynamism and Python being dynamically typed here, but instead I'll say that students gave me and my co-IA thank you cards they signed, and nominated us for awards that we weren't quite eligible for (not being grad students), so the school we worked for invented a new award for us -- but the students' high-fives and compliments to each other because they understood what the others were doing with APIs and Oauth tokens were the best part. "Amazing work," they started saying to each other -- and to <i>us</i> -- and I couldn't agree more. It isn't that there were no painful or frustrating parts -- holy, were there ever -- but that all of them were worth it, and I learnt a lot about how I'd deal with them if that were on me. Maybe someday. I'm only sorry I can't IA this class again.</p>

<p>Sappy as hell, OK. Who knows, that might've changed my career path. I guess I'll see.</p>
]]></content>
  </entry>
  
</feed>
